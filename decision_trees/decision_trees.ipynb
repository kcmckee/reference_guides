{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees in Python # \n",
    "\n",
    "Killian McKee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview ### \n",
    "\n",
    "1. [What are Decision Trees?](#section1)\n",
    "2. [Key Terms](#section2) \n",
    "3. [Pros and Cons of Decision Trees](#section3)\n",
    "4. [When to use Decision Trees](#section4)\n",
    "5. [Key Parameters](#section5) \n",
    "6. [Walkthrough: Building a Classification Tree](#section6) \n",
    "7. [Walkthrough: Building a Regression Tree](#section7)\n",
    "8. [Conclusion](#section8) \n",
    "9. [Additional Reading](#section9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are Decision Trees? ###\n",
    "\n",
    "Decision trees are a family of non-parametric algorithms capable of handling both classification and regression tasks. A decision tree takes input features and splits input data recursively based on those features (see example below). These splits ultimately lead to a shape resembling a tree with roots, nodes, and leaves (more on these later). \n",
    "\n",
    "<img src='dt_ex_simple.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terms ###\n",
    "\n",
    "This sections overviews the terminology we need to understand before diving more deeply into decision trees. \n",
    "\n",
    "**Root**: The top of the decision tree. This represents the most important variable related to answering our question being asked e.g. if our question is 'did a certain passenger survive their voyage on the titanic?', our root might have a split on the passenger's gender, since women were much more likely to survive than men.\n",
    "\n",
    "**Branches**: Further splits in the decision tree beneath the initial root.\n",
    "\n",
    "**Node**: Each split representing an 'either-or' scenario beneath the initial root. For example, in our titanic example our root question was 'male or female?', but a node beneath that might ask 'age>15?' with the branches 'yes' and 'no' as young children were put into lifeboats before adults.\n",
    "\n",
    "**Leaf**: The terminal node in a tree is a leaf\n",
    "\n",
    "**Pruning**: The act of removing nodes and branches from the tree in order to avoid overfitting and improve generalized performance\n",
    "\n",
    "**Purity**: Used by the algorithms that decide where to split a decision tree. A node is 100% impure when a node is split evenly and 100% pure when all of its data belongs to a single class. An effective decision tree model maximizes purity while avoiding impurity.\n",
    "\n",
    "**Gini Index**: The Gini index measures model purity by calculating how often randomly chosen elements are labeled incorrectly.The goal of the gini index is to reach 0, where the model is minimally impure and all the data falls into one decision at a split. \n",
    "\n",
    "**Information Gain**: the metric used to determine at which point to split each feature at each step in the tree. \n",
    "information gain = entropy(parent) - weighted sum of entropy(children). Maximizing information gain leads to a model with near perfect bias at the expense of very high variance (overfitting training data). To overcome these shortcomings we can employ pruning, boosting, and random forests (boosting and random forests discussed in separate guides). \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<img src='dt_example.png'>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Refresher on the Bias/Variance Trade Off ####\n",
    "<img src='bias_variance.png' width=\"400\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of Decision Trees ###\n",
    "\n",
    "Pros: \n",
    "\n",
    "1. **Easy to visualize**: Decision trees are easy to view schematically and can be displayed more easily than many algorithms. Understanding how a tree breaks down, where certain decisions are made, the impact of modifications to the algorithm, and the most critical features in a dataset can be viewed quickly with decision trees. \n",
    "2. **Flexible**: Decision trees can be used in both classification and regression tasks, can handle missing data more readily than many other algorithms, and can be easily pruned/tuned to alter model performance. \n",
    "3. **Fast**: Decision trees are fast to fit since they typically use greedy algorithms to fit the tree as opposed to fitting every possible tree. \n",
    "4. **Easy to implement**: Decision trees are straightforward to fit thanks to packages like scikit learn.\n",
    "\n",
    "Cons: \n",
    "\n",
    "1. **Prone to overfitting**: Decision trees are prone to overfitting. We can overcome this by using by pruning our trees or using a boosted decision tree or random forest (extensions of decision trees, discussed in separate guides). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to Use Decision Trees ###\n",
    "\n",
    "Decision trees are an appropriate starting point for most classification and regression tasks. They are especially valauble when you want to be able to visualize your model easily and understand its component pieces or when you need to make a model that runs very quickly. If your primary concern is accuracy and you have a solid of understanding of decision trees, random forests and boosted decision trees are superior extensions of the decision tree framework (covered in separate guides). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters ### \n",
    "\n",
    "There are 5 main levers we can move to influence our decision trees: \n",
    "\n",
    "1. Criterion: the function used to measure the quality of a split, the typical default is the gini index but alternatives will focus on things like information gain. \n",
    "2. Split: the strategy used to split a node, typically either 'best' or 'random'. Best typically leads to more accurate models, but random can help with overfitting. \n",
    "3. Max Depth: how deep the decision tree will grow. A deeper decision tree has lower variance but usually higher bias. \n",
    "4. Min Samples Split: the minimum number of samples required to split a node of our decision tree. Increasing this parameter constrains our decision tree by forcing it to consider more samples at each split. \n",
    "5. Min Samples Leaf: the minimum number of samples required to be at each leaf. Increasing this number can help smooth out the model (especially in regression), but constrains the growth of the model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Classification Decision Tree with Python ### \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be building a classifcation model to classify balance scale data from a generated psychological examination dataset.Our aim is to accurately classify whether something falls to the left or the right based on their exam answers. More info available on the data [here](http://archive.ics.uci.edu/ml/datasets/balance+scale). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import necessary packages \n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn import tree\n",
    "from sklearn.metrics import classification_report, confusion_matrix  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the data \n",
    "\n",
    "balance_data = pd.read_csv(\n",
    "'https://archive.ics.uci.edu/ml/machine-learning-databases/balance-scale/balance-scale.data',\n",
    "                           sep= ',', header= None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Lenght:  625\n",
      "Dataset Shape:  (625, 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>R</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   0  1  2  3  4\n",
       "0  B  1  1  1  1\n",
       "1  R  1  1  1  2\n",
       "2  R  1  1  1  3\n",
       "3  R  1  1  1  4\n",
       "4  R  1  1  1  5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# intial dataset examination\n",
    "\n",
    "print(\"Dataset Lenght: \", len(balance_data))\n",
    "print(\"Dataset Shape: \", balance_data.shape)\n",
    "balance_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# next, we split the data into features and our target variable so that we can split it into training and testing data \n",
    "\n",
    "x = balance_data.values[:, 1:5]\n",
    "y = balance_data.values[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Now we perform the train-test split on the data \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split( x, y, test_size = 0.3, random_state = 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=10,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=5, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=100,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we fit the decision classifier with the gini criterion selected\n",
    "\n",
    "clf_gini = DecisionTreeClassifier(criterion = \"gini\", random_state = 100,\n",
    "                               max_depth=10, min_samples_leaf=5)\n",
    "clf_gini.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L'], dtype=object)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# let's try and use our classifier to predict a single value \n",
    "\n",
    "clf_gini.predict([X_test[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['L', 'L', 'R', 'L', 'R', 'L', 'R', 'L', 'L', 'L', 'L', 'L', 'R',\n",
       "       'L', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'L', 'B', 'L', 'R', 'L',\n",
       "       'L', 'L', 'R', 'L', 'L', 'L', 'R', 'L', 'L', 'L', 'L', 'R', 'L',\n",
       "       'L', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'B', 'R', 'L', 'L', 'R',\n",
       "       'L', 'L', 'R', 'L', 'R', 'R', 'L', 'B', 'L', 'R', 'L', 'L', 'L',\n",
       "       'R', 'R', 'R', 'R', 'L', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R',\n",
       "       'R', 'L', 'R', 'R', 'L', 'R', 'L', 'R', 'R', 'L', 'R', 'L', 'R',\n",
       "       'R', 'L', 'L', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'L', 'R', 'R',\n",
       "       'R', 'R', 'R', 'R', 'L', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'R',\n",
       "       'R', 'R', 'R', 'L', 'R', 'R', 'L', 'L', 'L', 'L', 'L', 'L', 'R',\n",
       "       'R', 'R', 'R', 'L', 'R', 'R', 'R', 'L', 'L', 'R', 'L', 'L', 'L',\n",
       "       'R', 'L', 'L', 'R', 'L', 'R', 'R', 'L', 'L', 'L', 'R', 'R', 'R',\n",
       "       'L', 'R', 'R', 'B', 'R', 'R', 'B', 'L', 'R', 'R', 'L', 'R', 'R',\n",
       "       'R', 'L', 'R', 'B', 'R', 'L', 'R', 'R', 'L', 'R', 'R', 'B', 'R',\n",
       "       'R', 'L', 'L', 'R', 'R', 'R'], dtype=object)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now let's predict on the entire test set \n",
    "\n",
    "y_pred=clf_gini.predict(X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  3 10]\n",
      " [ 2 77  6]\n",
      " [ 5  8 77]]\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          B       0.00      0.00      0.00        13\n",
      "          L       0.88      0.91      0.89        85\n",
      "          R       0.83      0.86      0.84        90\n",
      "\n",
      "avg / total       0.79      0.82      0.81       188\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#let's see how the model performed by creating a confusion matrix and a classification report\n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the cell above, we can see that our model had an accuracy around 82%, which is acceptable on a dataset this small and with minimal parameter tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Regression Decision Tree with Python ### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough we will build a decision tree regressor to predict home prices in boston based on data such as the crime rate, average age, taxes, etc. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "\n",
    "from sklearn.datasets import load_boston\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import tree,metrics\n",
    "import numpy as np \n",
    "import pandas as pd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the dataset \n",
    "\n",
    "boston=load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Boston House Prices dataset\n",
      "===========================\n",
      "\n",
      "Notes\n",
      "------\n",
      "Data Set Characteristics:  \n",
      "\n",
      "    :Number of Instances: 506 \n",
      "\n",
      "    :Number of Attributes: 13 numeric/categorical predictive\n",
      "    \n",
      "    :Median Value (attribute 14) is usually the target\n",
      "\n",
      "    :Attribute Information (in order):\n",
      "        - CRIM     per capita crime rate by town\n",
      "        - ZN       proportion of residential land zoned for lots over 25,000 sq.ft.\n",
      "        - INDUS    proportion of non-retail business acres per town\n",
      "        - CHAS     Charles River dummy variable (= 1 if tract bounds river; 0 otherwise)\n",
      "        - NOX      nitric oxides concentration (parts per 10 million)\n",
      "        - RM       average number of rooms per dwelling\n",
      "        - AGE      proportion of owner-occupied units built prior to 1940\n",
      "        - DIS      weighted distances to five Boston employment centres\n",
      "        - RAD      index of accessibility to radial highways\n",
      "        - TAX      full-value property-tax rate per $10,000\n",
      "        - PTRATIO  pupil-teacher ratio by town\n",
      "        - B        1000(Bk - 0.63)^2 where Bk is the proportion of blacks by town\n",
      "        - LSTAT    % lower status of the population\n",
      "        - MEDV     Median value of owner-occupied homes in $1000's\n",
      "\n",
      "    :Missing Attribute Values: None\n",
      "\n",
      "    :Creator: Harrison, D. and Rubinfeld, D.L.\n",
      "\n",
      "This is a copy of UCI ML housing dataset.\n",
      "http://archive.ics.uci.edu/ml/datasets/Housing\n",
      "\n",
      "\n",
      "This dataset was taken from the StatLib library which is maintained at Carnegie Mellon University.\n",
      "\n",
      "The Boston house-price data of Harrison, D. and Rubinfeld, D.L. 'Hedonic\n",
      "prices and the demand for clean air', J. Environ. Economics & Management,\n",
      "vol.5, 81-102, 1978.   Used in Belsley, Kuh & Welsch, 'Regression diagnostics\n",
      "...', Wiley, 1980.   N.B. Various transformations are used in the table on\n",
      "pages 244-261 of the latter.\n",
      "\n",
      "The Boston house-price data has been used in many machine learning papers that address regression\n",
      "problems.   \n",
      "     \n",
      "**References**\n",
      "\n",
      "   - Belsley, Kuh & Welsch, 'Regression diagnostics: Identifying Influential Data and Sources of Collinearity', Wiley, 1980. 244-261.\n",
      "   - Quinlan,R. (1993). Combining Instance-Based and Model-Based Learning. In Proceedings on the Tenth International Conference of Machine Learning, 236-243, University of Massachusetts, Amherst. Morgan Kaufmann.\n",
      "   - many more! (see http://archive.ics.uci.edu/ml/datasets/Housing)\n",
      "\n",
      "these are the feature columns ['CRIM' 'ZN' 'INDUS' 'CHAS' 'NOX' 'RM' 'AGE' 'DIS' 'RAD' 'TAX' 'PTRATIO'\n",
      " 'B' 'LSTAT']\n",
      "\n",
      " this is the data:\n",
      " [[6.3200e-03 1.8000e+01 2.3100e+00 ... 1.5300e+01 3.9690e+02 4.9800e+00]\n",
      " [2.7310e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9690e+02 9.1400e+00]\n",
      " [2.7290e-02 0.0000e+00 7.0700e+00 ... 1.7800e+01 3.9283e+02 4.0300e+00]\n",
      " ...\n",
      " [6.0760e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 5.6400e+00]\n",
      " [1.0959e-01 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9345e+02 6.4800e+00]\n",
      " [4.7410e-02 0.0000e+00 1.1930e+01 ... 2.1000e+01 3.9690e+02 7.8800e+00]]\n",
      "\n",
      " this is the target: [24.  21.6 34.7 33.4 36.2 28.7 22.9 27.1 16.5 18.9 15.  18.9 21.7 20.4\n",
      " 18.2 19.9 23.1 17.5 20.2 18.2 13.6 19.6 15.2 14.5 15.6 13.9 16.6 14.8\n",
      " 18.4 21.  12.7 14.5 13.2 13.1 13.5 18.9 20.  21.  24.7 30.8 34.9 26.6\n",
      " 25.3 24.7 21.2 19.3 20.  16.6 14.4 19.4 19.7 20.5 25.  23.4 18.9 35.4\n",
      " 24.7 31.6 23.3 19.6 18.7 16.  22.2 25.  33.  23.5 19.4 22.  17.4 20.9\n",
      " 24.2 21.7 22.8 23.4 24.1 21.4 20.  20.8 21.2 20.3 28.  23.9 24.8 22.9\n",
      " 23.9 26.6 22.5 22.2 23.6 28.7 22.6 22.  22.9 25.  20.6 28.4 21.4 38.7\n",
      " 43.8 33.2 27.5 26.5 18.6 19.3 20.1 19.5 19.5 20.4 19.8 19.4 21.7 22.8\n",
      " 18.8 18.7 18.5 18.3 21.2 19.2 20.4 19.3 22.  20.3 20.5 17.3 18.8 21.4\n",
      " 15.7 16.2 18.  14.3 19.2 19.6 23.  18.4 15.6 18.1 17.4 17.1 13.3 17.8\n",
      " 14.  14.4 13.4 15.6 11.8 13.8 15.6 14.6 17.8 15.4 21.5 19.6 15.3 19.4\n",
      " 17.  15.6 13.1 41.3 24.3 23.3 27.  50.  50.  50.  22.7 25.  50.  23.8\n",
      " 23.8 22.3 17.4 19.1 23.1 23.6 22.6 29.4 23.2 24.6 29.9 37.2 39.8 36.2\n",
      " 37.9 32.5 26.4 29.6 50.  32.  29.8 34.9 37.  30.5 36.4 31.1 29.1 50.\n",
      " 33.3 30.3 34.6 34.9 32.9 24.1 42.3 48.5 50.  22.6 24.4 22.5 24.4 20.\n",
      " 21.7 19.3 22.4 28.1 23.7 25.  23.3 28.7 21.5 23.  26.7 21.7 27.5 30.1\n",
      " 44.8 50.  37.6 31.6 46.7 31.5 24.3 31.7 41.7 48.3 29.  24.  25.1 31.5\n",
      " 23.7 23.3 22.  20.1 22.2 23.7 17.6 18.5 24.3 20.5 24.5 26.2 24.4 24.8\n",
      " 29.6 42.8 21.9 20.9 44.  50.  36.  30.1 33.8 43.1 48.8 31.  36.5 22.8\n",
      " 30.7 50.  43.5 20.7 21.1 25.2 24.4 35.2 32.4 32.  33.2 33.1 29.1 35.1\n",
      " 45.4 35.4 46.  50.  32.2 22.  20.1 23.2 22.3 24.8 28.5 37.3 27.9 23.9\n",
      " 21.7 28.6 27.1 20.3 22.5 29.  24.8 22.  26.4 33.1 36.1 28.4 33.4 28.2\n",
      " 22.8 20.3 16.1 22.1 19.4 21.6 23.8 16.2 17.8 19.8 23.1 21.  23.8 23.1\n",
      " 20.4 18.5 25.  24.6 23.  22.2 19.3 22.6 19.8 17.1 19.4 22.2 20.7 21.1\n",
      " 19.5 18.5 20.6 19.  18.7 32.7 16.5 23.9 31.2 17.5 17.2 23.1 24.5 26.6\n",
      " 22.9 24.1 18.6 30.1 18.2 20.6 17.8 21.7 22.7 22.6 25.  19.9 20.8 16.8\n",
      " 21.9 27.5 21.9 23.1 50.  50.  50.  50.  50.  13.8 13.8 15.  13.9 13.3\n",
      " 13.1 10.2 10.4 10.9 11.3 12.3  8.8  7.2 10.5  7.4 10.2 11.5 15.1 23.2\n",
      "  9.7 13.8 12.7 13.1 12.5  8.5  5.   6.3  5.6  7.2 12.1  8.3  8.5  5.\n",
      " 11.9 27.9 17.2 27.5 15.  17.2 17.9 16.3  7.   7.2  7.5 10.4  8.8  8.4\n",
      " 16.7 14.2 20.8 13.4 11.7  8.3 10.2 10.9 11.   9.5 14.5 14.1 16.1 14.3\n",
      " 11.7 13.4  9.6  8.7  8.4 12.8 10.5 17.1 18.4 15.4 10.8 11.8 14.9 12.6\n",
      " 14.1 13.  13.4 15.2 16.1 17.8 14.9 14.1 12.7 13.5 14.9 20.  16.4 17.7\n",
      " 19.5 20.2 21.4 19.9 19.  19.1 19.1 20.1 19.9 19.6 23.2 29.8 13.8 13.3\n",
      " 16.7 12.  14.6 21.4 23.  23.7 25.  21.8 20.6 21.2 19.1 20.6 15.2  7.\n",
      "  8.1 13.6 20.1 21.8 24.5 23.1 19.7 18.3 21.2 17.5 16.8 22.4 20.6 23.9\n",
      " 22.  11.9]\n"
     ]
    }
   ],
   "source": [
    "# examine the dataset\n",
    "\n",
    "# sklearn documentation on the dataset\n",
    "print(boston.DESCR)\n",
    "\n",
    "# check out the column headers\n",
    "print('these are the feature columns', boston.feature_names)\n",
    "\n",
    "# check out the feature data \n",
    "print('\\n this is the data:\\n',boston['data'])\n",
    "\n",
    "# check out the target data \n",
    "print('\\n this is the target:',boston['target'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into our x and y columns \n",
    "\n",
    "x=boston.data\n",
    "y=boston.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# split the data into traniing and test sets \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeRegressor(criterion='mse', max_depth=None, max_features=None,\n",
       "           max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "           min_impurity_split=None, min_samples_leaf=10,\n",
       "           min_samples_split=30, min_weight_fraction_leaf=0.0,\n",
       "           presort=False, random_state=None, splitter='best')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# create and fit the regressor \n",
    "\n",
    "regressor = DecisionTreeRegressor(min_samples_split=30, min_samples_leaf=10)  \n",
    "regressor.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#generate some predictions \n",
    "\n",
    "y_pred = regressor.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Actual</th>\n",
       "      <th>Predicted</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>50.0</td>\n",
       "      <td>20.063636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>23.0</td>\n",
       "      <td>23.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8.3</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>21.2</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>19.9</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>20.6</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>18.7</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>16.1</td>\n",
       "      <td>20.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>18.6</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>8.8</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>17.2</td>\n",
       "      <td>15.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>14.9</td>\n",
       "      <td>14.317857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>10.5</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>50.0</td>\n",
       "      <td>44.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>29.0</td>\n",
       "      <td>32.745455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>23.0</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>33.3</td>\n",
       "      <td>32.745455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>29.4</td>\n",
       "      <td>26.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>21.0</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>23.8</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>19.1</td>\n",
       "      <td>20.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>20.4</td>\n",
       "      <td>20.063636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>29.1</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>19.3</td>\n",
       "      <td>20.929630</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>23.1</td>\n",
       "      <td>20.063636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>19.6</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>19.4</td>\n",
       "      <td>14.317857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>38.7</td>\n",
       "      <td>44.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>18.7</td>\n",
       "      <td>19.482143</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>23.5</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>31.2</td>\n",
       "      <td>38.390000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>74</th>\n",
       "      <td>23.7</td>\n",
       "      <td>29.808333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75</th>\n",
       "      <td>7.4</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>48.3</td>\n",
       "      <td>44.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>77</th>\n",
       "      <td>24.4</td>\n",
       "      <td>23.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>78</th>\n",
       "      <td>22.6</td>\n",
       "      <td>23.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>79</th>\n",
       "      <td>18.3</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>80</th>\n",
       "      <td>23.3</td>\n",
       "      <td>26.818182</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81</th>\n",
       "      <td>17.1</td>\n",
       "      <td>18.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>82</th>\n",
       "      <td>27.9</td>\n",
       "      <td>20.063636</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>83</th>\n",
       "      <td>44.8</td>\n",
       "      <td>44.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84</th>\n",
       "      <td>50.0</td>\n",
       "      <td>44.929167</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>85</th>\n",
       "      <td>23.0</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>86</th>\n",
       "      <td>21.4</td>\n",
       "      <td>23.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>87</th>\n",
       "      <td>10.2</td>\n",
       "      <td>15.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88</th>\n",
       "      <td>23.3</td>\n",
       "      <td>28.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>89</th>\n",
       "      <td>23.2</td>\n",
       "      <td>14.317857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>90</th>\n",
       "      <td>18.9</td>\n",
       "      <td>18.120000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>91</th>\n",
       "      <td>13.4</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92</th>\n",
       "      <td>21.9</td>\n",
       "      <td>21.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>93</th>\n",
       "      <td>24.8</td>\n",
       "      <td>29.808333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>94</th>\n",
       "      <td>11.9</td>\n",
       "      <td>28.540000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>95</th>\n",
       "      <td>24.3</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>96</th>\n",
       "      <td>13.8</td>\n",
       "      <td>9.265385</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>24.7</td>\n",
       "      <td>23.935714</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>98</th>\n",
       "      <td>14.1</td>\n",
       "      <td>14.317857</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>99</th>\n",
       "      <td>18.7</td>\n",
       "      <td>19.490476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>100</th>\n",
       "      <td>28.1</td>\n",
       "      <td>23.430769</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>101</th>\n",
       "      <td>19.8</td>\n",
       "      <td>21.272727</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>102 rows Ã— 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     Actual  Predicted\n",
       "0      22.6  23.935714\n",
       "1      50.0  20.063636\n",
       "2      23.0  23.430769\n",
       "3       8.3   9.265385\n",
       "4      21.2  21.272727\n",
       "5      19.9  19.490476\n",
       "6      20.6  21.272727\n",
       "7      18.7  19.490476\n",
       "8      16.1  20.929630\n",
       "9      18.6  21.272727\n",
       "10      8.8   9.265385\n",
       "11     17.2  15.300000\n",
       "12     14.9  14.317857\n",
       "13     10.5   9.265385\n",
       "14     50.0  44.929167\n",
       "15     29.0  32.745455\n",
       "16     23.0  21.272727\n",
       "17     33.3  32.745455\n",
       "18     29.4  26.818182\n",
       "19     21.0  19.490476\n",
       "20     23.8  23.935714\n",
       "21     19.1  20.929630\n",
       "22     20.4  20.063636\n",
       "23     29.1  23.935714\n",
       "24     19.3  20.929630\n",
       "25     23.1  20.063636\n",
       "26     19.6  19.490476\n",
       "27     19.4  14.317857\n",
       "28     38.7  44.929167\n",
       "29     18.7  19.482143\n",
       "..      ...        ...\n",
       "72     23.5  23.935714\n",
       "73     31.2  38.390000\n",
       "74     23.7  29.808333\n",
       "75      7.4   9.265385\n",
       "76     48.3  44.929167\n",
       "77     24.4  23.430769\n",
       "78     22.6  23.430769\n",
       "79     18.3  21.272727\n",
       "80     23.3  26.818182\n",
       "81     17.1  18.120000\n",
       "82     27.9  20.063636\n",
       "83     44.8  44.929167\n",
       "84     50.0  44.929167\n",
       "85     23.0  23.935714\n",
       "86     21.4  23.430769\n",
       "87     10.2  15.300000\n",
       "88     23.3  28.540000\n",
       "89     23.2  14.317857\n",
       "90     18.9  18.120000\n",
       "91     13.4   9.265385\n",
       "92     21.9  21.300000\n",
       "93     24.8  29.808333\n",
       "94     11.9  28.540000\n",
       "95     24.3  19.490476\n",
       "96     13.8   9.265385\n",
       "97     24.7  23.935714\n",
       "98     14.1  14.317857\n",
       "99     18.7  19.490476\n",
       "100    28.1  23.430769\n",
       "101    19.8  21.272727\n",
       "\n",
       "[102 rows x 2 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# comparing actual to predicted values, we see the model did a pretty good job\n",
    "\n",
    "df=pd.DataFrame({'Actual':y_test, 'Predicted':y_pred})  \n",
    "df  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Absolute Error: 3.171497451350393\n",
      "Mean Squared Error: 30.984776299315893\n",
      "Root Mean Squared Error: 5.566397066264308\n",
      "R^2: 0.6194845847927618\n"
     ]
    }
   ],
   "source": [
    "# getting the model metrics\n",
    "# we can see from the values below that our model did an ok, but not great job at making predictions. \n",
    "\n",
    "print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, y_pred))  \n",
    "print('Mean Squared Error:', metrics.mean_squared_error(y_test, y_pred))  \n",
    "print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, y_pred))) \n",
    "print('R^2:',metrics.r2_score(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ### \n",
    "\n",
    "In this guide we stepped through decision trees and learned they are useful for both classification and regression tasks. Next, we covered the key parameters,some pros and cons (beware of overfitting!), and key terminology we need to know to call ourselves proper data science arborists. Lastly, we walked through two basic classification and regression tree examples to see how people faired in a psychological exam and to predict housing prices respectively. Moving forward, check out the guides on boosted decision trees and random forests, which are more powerful extensions of the decision tree."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Reading ### \n",
    "\n",
    "1. Math behind decision trees [here](https://medium.com/@srnghn/the-mathematics-of-decision-trees-random-forest-and-feature-importance-in-scikit-learn-and-spark-f2861df67e3)\n",
    "2. More on the sklearn documentation [here](https://scikit-learn.org/stable/modules/tree.html)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources ### \n",
    "1. https://www.youtube.com/watch?v=BqOgaENTr08\n",
    "2. https://towardsdatascience.com/random-forests-and-the-bias-variance-tradeoff-3b77fee339b4\n",
    "3. http://www.cs.ubbcluj.ro/~gabis/DocDiplome/DT/DecisionTrees.pdf\n",
    "4. https://chrisalbon.com/machine_learning/trees_and_forests/random_forest_classifier_example/ \n",
    "5. https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html\n",
    "6. http://dataaspirant.com/2017/02/01/decision-tree-algorithm-python-with-scikit-learn/\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
