{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines (SVMs) in Python #\n",
    "##### Killian McKee #####"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Overview ###\n",
    "\n",
    "1. [What is a Support Vector Machine](#section1)\n",
    "2. [Pros and Cons of SVMs](#section2)\n",
    "3. [When to use SVMs](#section3)\n",
    "4. [Key Parameters](#section4)\n",
    "5. [SVM Classifier Walkthrough](#section5)\n",
    "6. [Conclusion](#section6)\n",
    "7. [Additional Reading](#section7)\n",
    "8. [Sources](#section8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What is a Support Vector Machine? ###\n",
    "\n",
    "The support vector machine (svm) is a supervised learning algorithm that is primarily used for classification (although it can be used for regression). Svms are currently one of the most popular machine learning algorithms because they are capable of performing non linear classifications on both standard and high dimensional data.  The objective of support vector machines is to find the optimal hyperplane in an N dimensional space (where N is the number of features). The optimal hyperplane has the largest margin between different classes of data. \n",
    "\n",
    "<img src='svm_example.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of SVMs ### \n",
    "\n",
    "#### Pros #### \n",
    "1. **Gives optimal solution**: Unlike some algorithms that can get caught in locally optimum solutions, svms always give the global optimum. \n",
    "2. **Overfitting Resistance**: svm's regularization parameters helps prevent overfitting.\n",
    "3. **Accurate**: svms are accurate and tunable thanks to their regularization and kernel parameters. \n",
    "4. **Resistant to Class Imbalances**: Svms continue to perform well on classification tasks where there are many more of a certain classs than the others. \n",
    "\n",
    "#### Cons ####\n",
    "1. **Slow Training Time**: Svm's can take a while to train on larger datasets. \n",
    "2. **Noise Sensitive**: Svm's can be adversely affected by noise in datasets, especially with overlapping classes. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use SVMs? ###\n",
    "\n",
    "Svms are a great option for most classification problems and should also be considered in regression. Because of their popularity, it is easier to talk about some of the instances when svms are not appropriate. \n",
    "\n",
    "1. They can be somewhat cumbersome for large multiclass classification problems since each class needs a new model \n",
    "2. On perceptual tasks (Speech, vision, etc.) svm's are usually worse the deep neural networks\n",
    "3. Gradient boosted trees tend to perform better on structured data than svms\n",
    "4. It can be difficult to interpret the output of svms\n",
    "5. They can take a long time to train on larger datasets \n",
    "6. Choosing a good [kernel function](https://towardsdatascience.com/understanding-the-kernel-trick-e0bc6112ef78) can be difficult"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key SVM Parameters ### \n",
    "\n",
    "There are three key parameters for svms: kernel, cost (lambda), and gamma\n",
    "\n",
    "1. Kernel: kernel is the type of svm we want to create (which could be linear, polynomial, sigmoid, or radial). We choose this based on the underlying shape of our data (can be tough to know without testing). For example, if we want to classify/separate nonlinear data, we wouldn't use a linear kernel. \n",
    "2. Lambda: serves as a degree of importance given to miss classifications of the svm. Higher lambda values necessitate more accurate models at the cost of generalizing on new data. \n",
    "3. Gamma: gamma is a parameter for gaussian kernels i.e. high dimensional data spaces (see below). Gamma controls the shapes of peaks in a high dimensional setting. A small gamma provides low bias with high variance and vice versa. Grid search can be used to find ideal lambda and gamma values. \n",
    "\n",
    "<img src='hdd_example.jpg'>\n",
    "                           "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SVM Classifier Walkthrough: Simple and Kernel ###\n",
    "\n",
    "Let's walk through how to build a svm classifier on some sample data. More specifically, we will walk through building a kernel svm. We will be using the iris dataset to classify species of iris based on their characteristics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# building a simple svm "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as mp \n",
    "import matplotlib.pyplot as plt \n",
    "import pandas as pd \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC  \n",
    "from sklearn.metrics import classification_report, confusion_matrix \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing the data \n",
    "\n",
    "#url for the dataset\n",
    "url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\"\n",
    "\n",
    "# Assign colum names to the dataset\n",
    "colnames = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'Class']\n",
    "\n",
    "# Read dataset to pandas dataframe\n",
    "irisdata = pd.read_csv(url, names=colnames)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preprocessing the data \n",
    "\n",
    "#drop class\n",
    "X = irisdata.drop('Class', axis=1)  \n",
    "y = irisdata['Class']  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#create a train test split \n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.20)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=8, gamma='auto', kernel='poly',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# here we implement a polynomial kernel svm using scikit learn\n",
    "#the degree parameter is the degree of the polynomial \n",
    "\n",
    "svclassifier = SVC(kernel='poly', degree=8)  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# making predictions on new test data\n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  0  4]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        12\n",
      "Iris-versicolor       1.00      0.93      0.96        14\n",
      " Iris-virginica       0.80      1.00      0.89         4\n",
      "\n",
      "    avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating our algorithm \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='rbf',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now lets build a svm with a gaussian kernel \n",
    "\n",
    "svclassifier = SVC(kernel='rbf')  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# getting predictions with our svm model \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[12  0  0]\n",
      " [ 0 13  1]\n",
      " [ 0  0  4]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       1.00      1.00      1.00        12\n",
      "Iris-versicolor       1.00      0.93      0.96        14\n",
      " Iris-virginica       0.80      1.00      0.89         4\n",
      "\n",
      "    avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluating the accuracy of the model \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(C=1.0, cache_size=200, class_weight=None, coef0=0.0,\n",
       "  decision_function_shape='ovr', degree=3, gamma='auto', kernel='sigmoid',\n",
       "  max_iter=-1, probability=False, random_state=None, shrinking=True,\n",
       "  tol=0.001, verbose=False)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#lets fit one last svm model with a sigmoid kernel \n",
    "\n",
    "svclassifier = SVC(kernel='sigmoid')  \n",
    "svclassifier.fit(X_train, y_train)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generating new predictions \n",
    "\n",
    "y_pred = svclassifier.predict(X_test)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0  0 12]\n",
      " [ 0  0 14]\n",
      " [ 0  0  4]]\n",
      "                 precision    recall  f1-score   support\n",
      "\n",
      "    Iris-setosa       0.00      0.00      0.00        12\n",
      "Iris-versicolor       0.00      0.00      0.00        14\n",
      " Iris-virginica       0.13      1.00      0.24         4\n",
      "\n",
      "    avg / total       0.02      0.13      0.03        30\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\598386\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\classification.py:1135: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "# model accuracy evaluation \n",
    "# here we can see the model is not very accurate when we make the assumption the data can be fit with a sigmoid svm \n",
    "\n",
    "print(confusion_matrix(y_test, y_pred))  \n",
    "print(classification_report(y_test, y_pred)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ### \n",
    "\n",
    "In this tutorial we stepped through how support vector machines can be used effectively in both classification and regression settings. Next, we examined the key parameters (kernel, lambda, and gamma) of svms and then fit three different svms to the iris dataset to classify different flower species. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Reading ### \n",
    "1. [The math behind svms](https://www.svm-tutorial.com/2014/11/svm-understanding-math-part-1/)\n",
    "2. [svms in depth](https://med.nyu.edu/chibi/sites/default/files/chibi/Final.pdf) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources ###\n",
    "\n",
    "1. https://www.quora.com/What-are-C-and-gamma-with-regards-to-a-support-vector-machine\n",
    "2. https://stackabuse.com/implementing-svm-and-kernel-svm-with-pythons-scikit-learn/\n",
    "3. https://www.google.com/search?rlz=1C1CHBD_enUS811US811&biw=1396&bih=641&tbm=isch&sa=1&ei=2tlIXIroIJKJjwTE4IGwCg&q=3d+data&oq=3d+data&gs_l=img.3..0i67l2j0l8.20519.21706..21839...1.0..0.74.332.5......1....1..gws-wiz-img.......0i7i30j0i8i7i30.6AmvZzYaikE#imgdii=E5kmTvFhCiHPlM:&imgrc=DhVTosm32bZMtM: \n",
    "4. https://www.google.com/search?rlz=1C1CHBD_enUS811US811&biw=1396&bih=641&tbm=isch&sa=1&ei=sLlIXL-ENei-jwSwib_IBg&q=support+vector+machine+optimal+hyperplane&oq=support+vector+machine+optimal+hyperplane&gs_l=img.3...13058.16730..16892...0.0..0.180.1768.15j4......1....1..gws-wiz-img.......0j0i8i30j0i24.-zmN7H2K3RU#imgrc=QhS3ivfEb21sNM:\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
