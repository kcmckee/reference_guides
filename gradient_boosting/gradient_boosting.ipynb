{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient Boosting with XGBoost in Python #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Killian McKee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview ##\n",
    "\n",
    "\n",
    "1. [What is Gradient Boosting?](#section1)\n",
    "2. [Key Terms](#section2) \n",
    "3. [Pros and Cons of Gradient Boosting Models](#section3)\n",
    "4. [When to use Gradient Boosting](#section4)\n",
    "5. [Key Parameters](#section5) \n",
    "6. [Walkthrough: Building an XGBoost Classifier](#section6)\n",
    "7. [Additional Reading](#section7) \n",
    "8. [Conclusion](#section8)\n",
    "9. [Sources](#section9)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is Gradient Boosting? ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient boosting models (gbm) are one of the fastest, accurate, and most widely used applications of a boosting model. Boosting is a method of turning many weak learning models into one strong model. Typically, these weaker models are decision trees because they are easy to understand, implement, and can provide highly accurate results. Similar to bagging techniques like random forests, boosting models choose a number of learners (in this case a number of trees) and assign them to training data with randomly initiated weights for each variable. The key difference between bagging and boosting models is how new trees i.e. learners are formed. In bagging, new learners are created independently and in parallel, while in boosting the trees are generated sequentially. Gradient boosting seeks to build increasingly accurate trees by using the output of past trees to tune the feature weights of future trees. Gradient boosted models assess the accuracy of past trees based on their gradient loss (measure of accuracy) and adjust feature weights using [gradient descent](https://towardsdatascience.com/gradient-descent-in-a-nutshell-eaf8c18212f0) optimization. The gif below demonstrates how a gradient descent optimization works to minimize loss. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='gradient_boosting.gif'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences Between Bagging and Boosting ###\n",
    "\n",
    "<img src='bagging_boosting.png'> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Terms ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Boosting**: A machine learning method to reduce model bias by sequentially combining weak performing models until one has a strong performing model. \n",
    "\n",
    "**Gradient Descent**: A macine learning optimization algorithm used during model training to change parameters with the goal of minimizing a cost function. \n",
    "\n",
    "**Root**: The top of the decision tree. This represents the most important variable related to answering our question being asked e.g. if our question is 'did a certain passenger survive their voyage on the titanic?', our root might have a split on the passenger's gender, since women were much more likely to survive than men (women and children were placed on lifeboats before men). \n",
    "\n",
    "**Branches**: Further splits in the decision tree beneath the initial root.\n",
    "\n",
    "**Node**: Each split representing an 'either-or' scenario beneath the initial root. For example, in our titanic example our root question was 'male or female?', but a node beneath that might ask 'age>15?' with the branches 'yes' and 'no' as young children were put into lifeboats before adults.\n",
    "\n",
    "**Leaf**: The terminal node in a tree is a leaf\n",
    "\n",
    "**Pruning**: The act of removing nodes and branches from the tree in order to avoid overfitting and improve generalized performance\n",
    "\n",
    "**Purity**: Used by the algorithms that decide where to split a decision tree. A node is 100% impure when a node is split evenly and 100% pure when all of its data belongs to a single class. An effective decision tree model maximizes purity while avoiding impurity.\n",
    "\n",
    "**Gini Index**: The Gini index measures model purity by calculating how often randomly chosen elements are labeled incorrectly.The goal of the gini index is to reach 0, where the model is minimally impure and all the data falls into one decision at a split. \n",
    "\n",
    "**Information Gain**: the metric used to determine at which point to split each feature at each step in a decision tree. \n",
    "information gain = entropy(parent) - weighted sum of entropy(children). Maximizing information gain leads to a model with near perfect bias at the expense of very high variance (overfitting training data). To overcome these shortcomings we can employ pruning, boosting, and random forests (boosting and random forests discussed in separate guides). \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pros and Cons ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros ###\n",
    "\n",
    "1. **Accuracy**:Gbms are very accurate and are frequently included in the ensemble models that win data science competitions. \n",
    "2. **Flexibility**: Gbms can optimize on many different cost functions and have many possible hyperparameters to tune. \n",
    "3. **Works with Subpar Data**: Gbms can handle missing data and a mix of categorical/numerical data, which reduces the need for data pre-processing. \n",
    "\n",
    "### Cons ###\n",
    "\n",
    "1. **Prone to Overfitting**: Gbms continuously work to minimize their cost function, which means they can be prone to overfitting. This can be mitigated by cross validation. \n",
    "2. **Computationally Expensive**: Gbms often require more than 1000 trees to get good results. Furthermore, gbms have so many hyperparameters that tuning tools like grid search can take a long time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use Gradient Boosting Models ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **When Accuracy is More Important than Training Speed**: Although gbms can be slower than alternatives like random forests, they are one of the most accurate machine learning algorithms currently utilized. \n",
    "2. **Classification, Regression, or Ranking**: Gbms can perform highly accurate rankings, regressions, and classifications. Consider using one for any one of these tasks.\n",
    "3. **You have the computational bandwith**: Gbms are resource intensive since models are built sequentially (as opposed to in parallel like with random forests) and there are many hyperparameters to adjust, but if have the ability to run one it should be a serious consideration for most problems. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Key Parameters ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **learning_rate**: The speed at which the model makes adjustments. A high learning rate can lead to overfitting, but too slow a learning rate can result in a model that takes too long to become accurate. \n",
    "2. **n_estimators**: The number of decision trees to use in our boosting model. More trees typically leads to higher accuracy (>1000), but causes the model to train more slowly. \n",
    "3. **max_depth**: The depth to which a decision tree can grow. A deeper tree can capture more information, but is more prone to overfitting and takes longer to train. \n",
    "4. **min_samples_split**: The minimum number of samples required to split an internal node.\n",
    "5. **min_samples_leaf** The minimum number of samples required to be at a leaf node for a split to occur. \n",
    "6. **max_features**: The number of features to consider when looking for the best split. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Walkthrough: Building an XGBoost Classifier ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our walkthrough will be building a classifier using the xgboost library (short for extreme gradient boosting) to classify different species of flowers from the iris dataset. First, we import our packages and data, then we will examine the data, split it into training and testing sets, perform a hyperparameter gridsearch, and then evaluate a cross validated model to get accuracy metrics. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import necessary packages \n",
    "\n",
    "import numpy as np \n",
    "import pandas as pd \n",
    "from sklearn import datasets \n",
    "import xgboost as xgb\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_curve, auc\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "\n",
    "iris=datasets.load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split the data into train and test sets\n",
    "\n",
    "X=iris.data\n",
    "y=iris.target\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=18)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Learning rate:  0.05\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n",
      "Learning rate:  0.1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n",
      "Learning rate:  0.25\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n",
      "Learning rate:  0.5\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n",
      "Learning rate:  0.75\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n",
      "Learning rate:  1\n",
      "Accuracy score (training): 1.000\n",
      "Accuracy score (validation): 1.000\n"
     ]
    }
   ],
   "source": [
    "# train with gb algorithm\n",
    "# test different learning rates \n",
    "# in a production or more complex setting, this is were one would implement grid search\n",
    "\n",
    "learning_rates = [0.05, 0.1, 0.25, 0.5, 0.75, 1]\n",
    "for learning_rate in learning_rates:\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate = learning_rate, max_features=2, max_depth = 2, random_state = 0)\n",
    "    gb.fit(X_train, y_train)\n",
    "    print(\"Learning rate: \", learning_rate)\n",
    "    print(\"Accuracy score (training): {0:.3f}\".format(gb.score(X_train, y_train)))\n",
    "    print(\"Accuracy score (validation): {0:.3f}\".format(gb.score(X_train, y_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "[[12  0  0]\n",
      " [ 0  8  0]\n",
      " [ 0  1  9]]\n",
      "\n",
      "Classification Report\n",
      "             precision    recall  f1-score   support\n",
      "\n",
      "          0       1.00      1.00      1.00        12\n",
      "          1       0.89      1.00      0.94         8\n",
      "          2       1.00      0.90      0.95        10\n",
      "\n",
      "avg / total       0.97      0.97      0.97        30\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# From above we can see our model is very accurate regardless of the learning rate \n",
    "\n",
    "gb = GradientBoostingClassifier(n_estimators=100, learning_rate = 0.5, max_features=3, max_depth = 2, random_state = 0)\n",
    "gb.fit(X_train, y_train)\n",
    "preds = gb.predict(X_test)\n",
    "\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, preds))\n",
    "print()\n",
    "print(\"Classification Report\")\n",
    "print(classification_report(y_test, preds))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Additional Reading ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Gradient Boosted Regression](http://uc-r.github.io/gbm_regression)\n",
    "2. [Additional Boosting Info](https://medium.com/greyatom/a-quick-guide-to-boosting-in-ml-acf7c1585cb5)\n",
    "3. [More on the math of GBM](https://www-bcf.usc.edu/~gareth/ISL/ISLR%20First%20Printing.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this walkthrough we tackled gradient boosting models. We learned that gradient boosting is one of the most powerful algorithms currently available, although it can be computationally expensive. Next, we discussed key model parameters, when to use the model, and pros of cons to using this type of model (powerful, not always the fastest/prone to overfitting). Lastly, we walked through how to build an iris classifier using extreme gradient boosting. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sources ##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. https://towardsdatascience.com/understanding-gradient-boosting-machines-9be756fe76ab\n",
    "2. https://en.wikipedia.org/wiki/Gradient_descent\n",
    "3. https://giphy.com/gifs/gradient-O9rcZVmRcEGqI\n",
    "4. http://uc-r.github.io/gbm_regression\n",
    "5. https://www.datacamp.com/community/tutorials/xgboost-in-python"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
