{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# K-Nearest Neighbors (KNN) in Python #"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Killian McKee"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [What is KNN?](#section1)\n",
    "2. [Key Terms](#section2) \n",
    "3. [Pros and Cons KNN](#section3)\n",
    "4. [When to use KNN](#section4)\n",
    "5. [Key Parameters](#section5) \n",
    "6. [Walkthrough: Building a KNN Classifier](#section6)\n",
    "7. [Additional Resources](#section7) \n",
    "8. [Conclusion](#section8)\n",
    "9. [Sources](#section9)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section1'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is KNN?##"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "K-nearest neighbors is a supervised, non-parametric classification algorithm. In other words, knn attempts to classify things into different pre-defined groups without making assumptions about the underlying shape of the data. Knn works by comparing the distance (euclidean, manhattan, etc.) of new data points to all existing data points. The algorithm then selects the k-nearest data points where k can be any integer (selected by the user). Lastly, the class to which the majority of the k nearest points belong is assigned to the new data point. We can see in the image below how changing the value of k can impact classification results; when k=1 the new example to classify (denoted by the green circle) would be put into class 1 since its only neighbor is in class 1 (blue square), but if k is increased to 3 then the new classification changes to class 2 since the triangle count>square count for the new circle. Using a larger value of k will lead to more things being classified as the most common class at the cost of capturing intricate decision boundaries. Conversely, small k values are more adversely affected by noise or variance in the data, but can capture variable shapes more accurately. Typically, optimal k-values are found by performing a train/test split on the data and then assessing model accuracy with different levels of k. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='knn.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section2'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Terms ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. **k**: a numerical value representing the distance (euclidean, manhattan, etc.) within which the knn model will look for neighbors to use in classification.\n",
    "2. **non-parametric**: makes no assumptions about the underlying data shape/distribution\n",
    "3. **clustering**: forming non-overlapping groups of similar objects"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section3'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pros and Cons of KNN ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Pros ####\n",
    "1. **Easy to Implement**: knn is a straightforward algorithm that is easy to implement, visualize, and understand. \n",
    "2. **No Required Training**:knn is a [lazy learning](https://en.wikipedia.org/wiki/Lazy_learning) algorithm, which means it doesn't need to train on data ahead of time to make new classifications. This means knn is very fast on small/medium sized datasets. \n",
    "3. **New Data Integrated Seamlessly**: since no data is needed for training, new data can be integrated more seamlessly into knn than most other clustering algorithms. \n",
    "\n",
    "#### Cons #### \n",
    "1. **HDD Shortcomings**: high dimesnional data (hdd), or data with many features can make it difficult for the knn algorithm to measure distances between points\n",
    "2. **Slow on Large Datasets**: knn does not scale up to very large datasets well. This is because it becomes increasingly costly to measure the distance between data points as the number of points increases. \n",
    "3. **Categorical Shortcomings**: knn doesn't handle categorical features well because it seeks works by comparing the distance between features. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section4'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use KNN ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knn works well at performing non-parametric clustering and classifications on small/medium sized datasets in which most of the features are not categorical. It is especially useful when one wants to continuously generate new classifications since it is a lazy learner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section5'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key Parameters ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**n_neighbors**: the number of neighbors to include in the classificiation\n",
    "**weights**: weight function used in prediction. Allows some points to be weighted more heavily than others e.g. close points are more important that far ones, etc. [More info](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html)\n",
    "**algorithm**: algorithm used to compute nearest neighbor. \"Auto\" is the most common choice because it attempts to find the best fit out of all the available options. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section6'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Walkthrough: Building a KNN Classifier ###\n",
    "\n",
    "This walkthrough steps through how to build a classifier on the [iris dataset](https://scikit-learn.org/stable/auto_examples/datasets/plot_iris_dataset.html). We will be attempting classify new observations as one of three possible iris species based on factors like their petal length and width"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import necessary packages \n",
    "\n",
    "import pandas as pd \n",
    "import numpy as np \n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load in the data and convert it from an array into a dataframe\n",
    "# the conversion is optional, but most datasets won't come in the scikit array format\n",
    "\n",
    "data=load_iris()\n",
    "df = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "df['target'] = data['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)  \\\n",
       "0                5.1               3.5                1.4               0.2   \n",
       "1                4.9               3.0                1.4               0.2   \n",
       "2                4.7               3.2                1.3               0.2   \n",
       "3                4.6               3.1                1.5               0.2   \n",
       "4                5.0               3.6                1.4               0.2   \n",
       "\n",
       "   target  \n",
       "0       0  \n",
       "1       0  \n",
       "2       0  \n",
       "3       0  \n",
       "4       0  "
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the top of the dataset \n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(150, 5)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# examine the shape of the data \n",
    "# 150 rows with 5 columns \n",
    "\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split our dataset into data and target (x&y)\n",
    "\n",
    "X=df.drop(columns='target')\n",
    "y=df['target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check our x dataset\n",
    "\n",
    "X.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    0\n",
       "1    0\n",
       "2    0\n",
       "3    0\n",
       "4    0\n",
       "Name: target, dtype: int32"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check our y dataset \n",
    "\n",
    "y.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.96666667 0.96666667 0.93333333 0.96666667 1.        ]\n",
      "cv_scores mean:0.9666666666666668\n"
     ]
    }
   ],
   "source": [
    "# we will be building and validating our model with kfold cross validation, which tends to be the most accurate method\n",
    "\n",
    "#initialize the model \n",
    "knn_cv = KNeighborsClassifier(n_neighbors=3,weights='uniform', algorithm='auto')\n",
    "\n",
    "#train model with cv of 5 \n",
    "cv_scores = cross_val_score(knn_cv, X, y, cv=5)\n",
    "\n",
    "#print each cv score (accuracy) and average them\n",
    "print(cv_scores)\n",
    "print('cv_scores mean:{}'.format(np.mean(cv_scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=1, n_neighbors=5, p=2,\n",
       "           weights='uniform'),\n",
       "       fit_params=None, iid=True, n_jobs=1,\n",
       "       param_grid={'n_neighbors': array([ 1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "       18, 19])},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring=None, verbose=0)"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# although we achieved an accuracy of 96%, lets do some hyperparameter tuning to find an optimal k value \n",
    "# gridsearchCV is an optimization method we can use to find the best parameters for a model \n",
    "\n",
    "#create new a knn model\n",
    "knn2 = KNeighborsClassifier()\n",
    "\n",
    "#create a dictionary of all values we want to test for n_neighbors\n",
    "param_grid = {'n_neighbors': np.arange(1, 20)}\n",
    "\n",
    "#use gridsearch to test all values for n_neighbors\n",
    "knn_gscv = GridSearchCV(knn2, param_grid, cv=5)\n",
    "\n",
    "#fit model to data\n",
    "knn_gscv.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "optimal neighbor parameter: {'n_neighbors': 6}\n"
     ]
    }
   ],
   "source": [
    "# display the best parameters for the model \n",
    "\n",
    "print('optimal neighbor parameter:',knn_gscv.best_params_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best accuracy: 0.98\n"
     ]
    }
   ],
   "source": [
    "# display the best accuracy achieved by our gridsearch model \n",
    "\n",
    "print('best accuracy:',knn_gscv.best_score_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section7'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Additional Resources ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. [Knn video explanation](https://www.youtube.com/watch?v=UqYde-LULfs)\n",
    "2. [Knn from scratch](https://medium.com/deep-math-machine-learning-ai/chapter-5-k-nearest-neighbors-algorithm-with-code-from-scratch-7f93f653c860)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section8'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conclusion ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this tutorial we stepped through knn, which is a supervised learning clustering algorithm that measures the distance between a new point and its nearest neighbors to form clusters. We also covered some of the pros and cons i.e. learned that it is a lazy learner, meaning it can quickly integrate new data without training, but that it struggles with categorical and high dimensional data. Next, we tackled use cases, parameters, and a walkthrough on how to implement a knn classifier with scikit learn. Those looking to go into greater depth should check out the knn from scratch link above, which steps through how to build a knn model without scikit. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id='section9'></a> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sources ###\n",
    "1. https://stackabuse.com/k-nearest-neighbors-algorithm-in-python-and-scikit-learn/\n",
    "2. https://medium.com/@adi.bronshtein/a-quick-introduction-to-k-nearest-neighbors-algorithm-62214cea29c7\n",
    "3. https://towardsdatascience.com/building-a-k-nearest-neighbors-k-nn-model-with-scikit-learn-51209555453a\n",
    "4. https://medium.com/deep-math-machine-learning-ai/chapter-5-k-nearest-neighbors-algorithm-with-code-from-scratch-7f93f653c860"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
